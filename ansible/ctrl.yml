# ctrl.yml
---
# Ansible playbook for control node
- name: Setting up control node from ctrl notebook
  hosts: ctrl
  become: true
  vars:
    apiserver_addr: 192.168.56.100
    pod_cidr: 10.244.0.0/16
    ctrl_name: ctrl
    kubeconfig_host_path: "../" # Hosts root project directry
    kubeconfig_user_path: "/home/vagrant/.kube/config"
    kubeconfig_src: "/etc/kubernetes/admin.conf"
    flannel_version: "v0.26.7"
    flannel_url: "https://raw.githubusercontent.com/flannel-io/flannel/{{ flannel_version }}/Documentation/kube-flannel.yml"
    flannel_tmp: "/tmp/kube-flannel-{{ flannel_version }}.yml"
    flannel_iface: "eth1"

  tasks:
    - import_tasks: general.yml

    - name: Check for /etc/kubernetes/admin.conf
      stat:
        path: "{{ kubeconfig_src }}"
      register: admin_conf

    # --- Block for Kubernetes Initialization (runs only if admin.conf is absent) ---
    - name: Initialize Kubernetes Control Plane and CNI
      block:
        - name: Run kubeadm init
          ansible.builtin.shell: |
            kubeadm init \
              --apiserver-advertise-address={{ apiserver_addr }} \
              --node-name={{ ctrl_name }} \
              --pod-network-cidr={{ pod_cidr }}
          args:
            creates: "{{ kubeconfig_src }}" 

        - name: Untaint control-plane so MetalLB (and others) can schedule
          shell: kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule- || true
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
    
        - name: Create .kube directory for vagrant user
          ansible.builtin.file:
            path: "{{ kubeconfig_user_path | dirname }}"
            state: directory
            owner: vagrant
            group: vagrant
            mode: '0755'

        - name: Copy admin.conf to vagrant user's .kube
          ansible.builtin.copy:
            src: "{{ kubeconfig_src }}"
            dest: "{{ kubeconfig_user_path }}"
            remote_src: true
            owner: vagrant
            group: vagrant
            mode: '0644'

        - name: Publish admin.conf to host via /vagrant
          ansible.builtin.copy:
            src: "{{ kubeconfig_src }}"
            dest: "/vagrant/admin.conf"
            remote_src: true
            mode: '0644'
          ignore_errors: true

        - name: Wait for API server to be initially available after kubeadm init
          ansible.builtin.wait_for:
            host: "{{ apiserver_addr }}"
            port: 6443
            delay: 10  
            timeout: 180 
            state: started 
          vars:
            ansible_python_interpreter: /usr/bin/python3 

        - name: Download Flannel manifest (if not already present)
          ansible.builtin.get_url:
            url: "{{ flannel_url }}"
            dest: "{{ flannel_tmp }}"
            mode: '0644'
            force: no 

        - name: Inject --iface into Flannel DaemonSet args
          ansible.builtin.replace:
            path: "{{ flannel_tmp }}"
            regexp: '^(\s*)- --ip-masq$'
            replace: '\1- --ip-masq\n\1- --iface={{ flannel_iface }}'

        - name: Apply Flannel network manifest
          ansible.builtin.command: 
            cmd: "kubectl apply -f {{ flannel_tmp }} --kubeconfig {{ kubeconfig_src }}"
          changed_when: true 

        - name: Wait for Flannel pods to be ready
          ansible.builtin.command:
            cmd: "kubectl wait --for=condition=ready pod -l app=flannel -n kube-flannel --timeout=300s --kubeconfig {{ kubeconfig_src }}"
          register: flannel_status
          until: flannel_status.rc == 0
          retries: 6
          delay: 30
          changed_when: false 

      when: not admin_conf.stat.exists 

    - name: Wait for CoreDNS pods in kube-system to be ready
      ansible.builtin.command:
        cmd: "kubectl wait --for=condition=ready pod -l k8s-app=kube-dns -n kube-system --timeout=300s --kubeconfig {{ kubeconfig_src }}"
      register: coredns_status
      until: coredns_status.rc == 0
      retries: 6
      delay: 20
      changed_when: false

    # --- Helm and Application Stack Deployment ---
    - name: Add Helm signing key
      ansible.builtin.apt_key:
        url: https://baltocdn.com/helm/signing.asc
        state: present

    - name: Add Helm apt repository
      ansible.builtin.apt_repository:
        repo: deb https://baltocdn.com/helm/stable/debian/ all main
        state: present
        filename: helm-stable

    - name: Update apt cache (runs if cache is older than 1 hour)
      ansible.builtin.apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install Helm package
      ansible.builtin.apt:
        name: helm
        state: present

    - name: Get list of installed helm plugins
      ansible.builtin.command: helm plugin list
      register: helm_plugins
      changed_when: false
      environment: 
        KUBECONFIG: "{{ kubeconfig_user_path }}" 

    - name: Install helm-diff plugin if not already installed
      ansible.builtin.command: helm plugin install https://github.com/databus23/helm-diff
      when: "'diff' not in helm_plugins.stdout"
      environment:
        KUBECONFIG: "{{ kubeconfig_user_path }}"

    - name: Add Prometheus community Helm repository
      kubernetes.core.helm_repository: 
        name: prometheus-community
        repo_url: https://prometheus-community.github.io/helm-charts
        state: present

    - name: Deploy kube-prometheus-stack
      kubernetes.core.helm:
        atomic: true
        name: prometheus-stack # DO NOT CHANGE THIS
        chart_ref: prometheus-community/kube-prometheus-stack
        release_namespace: monitoring
        create_namespace: true 
        kubeconfig: "{{ kubeconfig_src }}"
        state: present 
        update_repo_cache: true
        values: 
          fullnameOverride: "kube-prometheus"

          prometheus-node-exporter:
            tolerations: # Black magic necessary to make it all work
            - key: "node-role.kubernetes.io/control-plane"
              operator: "Exists" 
              effect: "NoSchedule" 

          # Grafana Configuration
          grafana:
            enabled: true
            adminUser: admin           
            adminPassword: admin     
            tolerations: # Black magic necessary to make it all work
            - key: "node-role.kubernetes.io/control-plane"
              operator: "Exists" 
              effect: "NoSchedule" 
            sidecar:
              dashboards:
                enabled: true
                label: "grafana_dashboard"
                labelValue: "1"
            service:
              type: NodePort
            ingress:
              enabled: false
              #ingressClassName: nginx
              #paths:
              #  - /grafana
              #pathType: Prefix

          # Prometheus Configuration
          prometheus:
            enabled: true
            prometheusSpec:
              serviceMonitorNamespaceSelector: {}
              serviceMonitorSelector: {}
              tolerations: # Black magic 
                - key: "node-role.kubernetes.io/control-plane"
                  operator: "Exists"
                  effect: "NoSchedule"
            service:
              type: NodePort
            ingress:
              enabled: false
              #ingressClassName: nginx
              #paths:
              #  - /prometheus
              #pathType: Prefix

          prometheusOperator:
            tolerations: # Black magic necessary to make it all work
            - key: "node-role.kubernetes.io/control-plane"
              operator: "Exists" 
              effect: "NoSchedule" 
            admissionWebhooks: # This is necessary due to some stupid bug 
              patch:
                tolerations:
                - key: "node-role.kubernetes.io/control-plane"
                  operator: "Exists"
                  effect: "NoSchedule"

            #  enabled: false
            #  patch:
            #    enabled: false
            #tls:
            #  enabled: false
            #tlsProxy:
            #  enabled: false
            #certManager:
            #  enabled: true

          kube-state-metrics:
            tolerations:
            - key: "node-role.kubernetes.io/control-plane"
              operator: "Exists"
              effect: "NoSchedule"

          alertmanager:
            enabled: false


